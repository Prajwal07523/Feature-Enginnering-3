{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5646bfe6",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its \n",
    "application.\n",
    "\n",
    "Min-Max scaling is a data normalization technique used in data preprocessing to rescale the feature values to a fixed range. It scales the features to a range of 0 to 1, where 0 represents the minimum value of the feature, and 1 represents the maximum value of the feature. This method is also known as normalization.\n",
    "\n",
    "The formula to compute the Min-Max scaling is as follows:\n",
    "\n",
    "x_scaled = (x - x_min) / (x_max - x_min)\n",
    "\n",
    "where x is the original feature value, x_min is the minimum value of the feature, and x_max is the maximum value of the feature.\n",
    "\n",
    "x1\tx2\n",
    "\n",
    "2\t100\n",
    "\n",
    "4\t200\n",
    "\n",
    "6\t300\n",
    "\n",
    "after min_max scaling\n",
    "\n",
    "x1\tx2\tx1_scaled\tx2_scaled\n",
    "\n",
    "2\t100\t  0.0  \t     0.0\n",
    "\n",
    "4\t200\t 0.5\t     0.5\n",
    "\n",
    "6\t300\t 1.0\t     1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc5d5ca",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? \n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "The Unit Vector technique, also known as the L2 normalization technique, is a feature scaling method used to transform input features into a standardized scale. It involves scaling each feature to have a unit norm or a magnitude of 1.\n",
    "\n",
    "Compared to Min-Max scaling, which scales the data to a fixed range of values (usually between 0 and 1), the Unit Vector technique maintains the direction of the data points while scaling them to the same scale. This is particularly useful in cases where the direction or angle between feature vectors is important, such as in clustering or classification tasks.\n",
    "\n",
    "For example, suppose we have a dataset of two-dimensional points:\n",
    "[(2, 3), (4, 5), (6, 7), (8, 9)]\n",
    "\n",
    "To apply the Unit Vector technique to this dataset, we first calculate the L2 norm of each feature vector \n",
    "||(2, 3)|| = sqrt(2^2 + 3^2) = 3.61\n",
    "||(4, 5)|| = sqrt(4^2 + 5^2) = 6.40\n",
    "||(6, 7)|| = sqrt(6^2 + 7^2) = 9.22\n",
    "||(8, 9)|| = sqrt(8^2 + 9^2) = 12.04\n",
    "\n",
    "Then, we divide each feature vector by its corresponding L2 norm:\n",
    "[(2/3.61, 3/3.61), (4/6.40, 5/6.40), (6/9.22, 7/9.22), (8/12.04, 9/12.04)]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0ca595",
   "metadata": {},
   "source": [
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an \n",
    "example to illustrate its application.\n",
    "\n",
    "Principle Component Analysis (PCA) is a widely used statistical technique for data analysis and dimensionality reduction. PCA is a mathematical method that transforms a set of correlated variables into a new set of uncorrelated variables called principal components. The principal components represent the underlying structure of the data in terms of its variance and covariance.\n",
    "\n",
    "PCA is used for dimensionality reduction, where high-dimensional data is transformed into a lower-dimensional space while preserving as much of the variability as possible. This is achieved by selecting a subset of the principal components that capture most of the variance in the data. The reduced dataset can then be visualized or used in subsequent analysis, such as clustering or classification.\n",
    "\n",
    "Here is an example to illustrate the application of PCA for dimensionality reduction. Suppose we have a dataset containing the following four variables: height, weight, age, and income of individuals. The dataset has a large number of observations, and we want to reduce the number of variables to two for easier visualization.\n",
    "\n",
    "To do this, we can apply PCA to the dataset. First, we standardize the data to make each variable have a mean of zero and unit variance. Then, we compute the covariance matrix of the standardized data. The eigenvectors and eigenvalues of the covariance matrix are then computed, and the eigenvectors corresponding to the largest eigenvalues are selected as the principal components. In this example, let us say that the first two principal components explain 80% of the variance in the data.\n",
    "\n",
    "We can then transform the data into the new two-dimensional space defined by the first two principal components. Each observation in the original dataset is now represented by a two-dimensional point, where the x-axis corresponds to the first principal component and the y-axis corresponds to the second principal component. This reduced dataset can be visualized using a scatter plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65b489f",
   "metadata": {},
   "source": [
    "Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature \n",
    "Extraction? Provide an example to illustrate this concept.\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction by transforming the original high-dimensional data into a lower dimensional space. Feature extraction is the process of extracting relevant features from the input data and transforming them into a lower dimensional space.\n",
    "\n",
    "PCA can be used for feature extraction by identifying the most important features or components of the original data and reducing the dimensionality of the data by projecting it onto these components. The extracted features can then be used for further analysis or modeling.\n",
    "\n",
    "For example, suppose we have a dataset with 100 features and we want to reduce the dimensionality of the data to improve the efficiency of a machine learning algorithm. We can use PCA to extract the most important features by identifying the principal components that explain the most variance in the data. We can then project the original data onto these principal components to obtain a new dataset with a lower dimensionality"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29182c64",
   "metadata": {},
   "source": [
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset \n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to \n",
    "preprocess the data\n",
    "\n",
    "Min-Max scaling is a common technique used to normalize the range of a feature, so that it falls within a specific range, typically between 0 and 1. The goal of this normalization is to ensure that features with vastly different scales are given equal weight during model training.\n",
    "\n",
    "To use Min-Max scaling to preprocess the data for a food delivery recommendation system, the following steps can be taken:\n",
    "\n",
    "Identify the numerical features in the dataset that need to be scaled. In this case, we could use price, rating, and delivery time.\n",
    "\n",
    "Determine the minimum and maximum values for each feature.\n",
    "\n",
    "Use the following formula to scale each feature to a value between 0 and 1:\n",
    "\n",
    "scaled_feature = (original_feature - min_feature) / (max_feature - min_feature)\n",
    "\n",
    "Repeat step 3 for each numerical feature in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f247e25",
   "metadata": {},
   "source": [
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many \n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the \n",
    "dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a575283b",
   "metadata": {},
   "source": [
    "7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the \n",
    "values to a range of -1 to 1.\n",
    "\n",
    "X_new = (X - X_min) / (X_max - X_min) * (max_range - min_range) + min_range\n",
    "\n",
    "X_min = 1\n",
    "X_max = 20\n",
    "max_range = -1\n",
    "min_range = 1\n",
    "\n",
    "X_new1 = (1 - 1) / (20 - 1) * (-1 - 1) + 1 = 1\n",
    "\n",
    "X_new2 = (5 - 1) / (20 - 1) * (-1 - 1) + 1 = 0.2\n",
    "\n",
    "X_new3 = (10 - 1) / (20 - 1) * (-1 - 1) + 1 = -0.2\n",
    "\n",
    "X_new4 = (15 - 1) / (20 - 1) * (-1 - 1) + 1 = -0.6\n",
    "\n",
    "X_new5 = (20 - 1) / (20 - 1) * (-1 - 1) + 1 = -1\n",
    "\n",
    "Therefore, the Min-Max scaled values for the given dataset in the range of -1 to 1 are [1, 0.2, -0.2, -0.6, -1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442b5b57",
   "metadata": {},
   "source": [
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform \n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?\n",
    "\n",
    "To perform feature extraction using PCA, we first need to standardize the data to have zero mean and unit variance. Then we can apply PCA to the standardized data and obtain the principal components. The number of principal components to retain depends on the amount of variance we want to explain and the desired level of dimensionality reduction.\n",
    "\n",
    "Without knowing more about the dataset and its specific characteristics, it's difficult to determine exactly how many principal components to retain. However, as a general rule of thumb, we might choose to retain enough principal components to explain at least 80% or 90% of the variance in the data. This ensures that we capture the most important patterns in the data while still reducing the dimensionality.\n",
    "\n",
    "To decide on the number of principal components to retain, we can examine the explained variance ratio of each component and choose the smallest number of components that explain at least 80% or 90% of the variance. Alternatively, we can use a scree plot to visualize the amount of variance explained by each component and choose the \"elbow\" point where adding more components does not result in a significant increase in explained variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d760817e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
